{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, valid=0.1, test=0.1):\n",
    "    if test == 0 and valid == 0:\n",
    "        return data\n",
    "    \n",
    "    x = dat\n",
    "    size = len(x)\n",
    "    \n",
    "    # split\n",
    "    train_features = x[:int(-(size * (test+valid)))]\n",
    "    valid_features= x[int(-(size * (test+valid))): int(-size*test)]\n",
    "    test_features = x[int(-size*test):]\n",
    "    \n",
    "    return train_features, valid_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time_dataset(data, test=0.1):\n",
    "    x = data\n",
    "    \n",
    "    # split into 9 years of train and 1 year of test\n",
    "    train_features = x[:-8760]\n",
    "    test_features = x[-8760:]\n",
    "    \n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(train_data, valid_data, test_data, source_path, param, dirr):\n",
    "    # saving dataset as parts\n",
    "    \n",
    "    if train_data:\n",
    "        train_name = source_path + 'train_' + param + '_' + dirr + '.pkl'\n",
    "        with open(train_name,'wb') as f:\n",
    "            pickle.dump(train_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "            print('saved ' + train_name)\n",
    "            \n",
    "    if test_data:\n",
    "        test_name = source_path + 'test_' + param + '_' + dirr + '.pkl'\n",
    "        with open(test_name,'wb') as f:\n",
    "            pickle.dump(test_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "            print('saved ' + test_name)\n",
    "\n",
    "    if valid_data:\n",
    "        valid_name = source_path + 'valid_' + param + '_' + dirr + '.pkl'\n",
    "        with open(valid_name,'wb') as f:\n",
    "            pickle.dump(valid_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "            print('saved ' + valid_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirr = '32x32'\n",
    "dirr = '64x64'\n",
    "\n",
    "params = ['Temperature', 'Specific_humidity', 'Cloud_cover']\n",
    "params_64 = ['Geopotential', 'Logarithm_of_surface_pressure']\n",
    "\n",
    "# path to whole .pkl of data\n",
    "source_path = '../PARSED/' + dirr + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in params:\n",
    "#     fin_file = param + '.pkl'\n",
    "#     \n",
    "#    # loading dataset\n",
    "#     with open(source_time_path + fin_file, 'rb') as f:\n",
    "#        data = pickle.load(f, encoding='bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80:10:10\n",
    "# data_train, data_valid, data_test = split_dataset(data, valid=0.1, test=0.1)\n",
    "# save_dataset(data_train, data_valid, data_test, source_path, params, dirr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature.pkl\n",
      "78912\n",
      "Tue Jan  1 00:00:00 2008\n",
      "8760\n",
      "Sun Jan  1 00:00:00 2017\n",
      "saved ../PARSED/64x64/train_Temperature_64x64.pkl\n",
      "saved ../PARSED/64x64/test_Temperature_64x64.pkl\n",
      "Specific_humidity.pkl\n",
      "78912\n",
      "Tue Jan  1 00:00:00 2008\n",
      "8760\n",
      "Sun Jan  1 00:00:00 2017\n",
      "saved ../PARSED/64x64/train_Specific_humidity_64x64.pkl\n",
      "saved ../PARSED/64x64/test_Specific_humidity_64x64.pkl\n",
      "Cloud_cover.pkl\n",
      "78912\n",
      "Tue Jan  1 00:00:00 2008\n",
      "8760\n",
      "Sun Jan  1 00:00:00 2017\n",
      "saved ../PARSED/64x64/train_Cloud_cover_64x64.pkl\n",
      "saved ../PARSED/64x64/test_Cloud_cover_64x64.pkl\n",
      "Geopotential.pkl\n",
      "78912\n",
      "Tue Jan  1 00:00:00 2008\n",
      "8760\n",
      "Sun Jan  1 00:00:00 2017\n",
      "saved ../PARSED/64x64/train_Geopotential_64x64.pkl\n",
      "saved ../PARSED/64x64/test_Geopotential_64x64.pkl\n",
      "Logarithm_of_surface_pressure.pkl\n",
      "78912\n",
      "Tue Jan  1 00:00:00 2008\n",
      "8760\n",
      "Sun Jan  1 00:00:00 2017\n",
      "saved ../PARSED/64x64/train_Logarithm_of_surface_pressure_64x64.pkl\n",
      "saved ../PARSED/64x64/test_Logarithm_of_surface_pressure_64x64.pkl\n"
     ]
    }
   ],
   "source": [
    "for param in params:\n",
    "    fin_file = param + '.pkl'\n",
    "    \n",
    "    print(fin_file)\n",
    "    # loading dataset\n",
    "    with open(source_path + fin_file, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='bytes')\n",
    "\n",
    "    # no valid - 90:10\n",
    "    \n",
    "    # NO TIME\n",
    "    # data_train, data_valid, data_test = split_dataset(data, valid=0, test=0.1)\n",
    "    # save_dataset(data_train, data_valid, data_test, source_path, params, dirr)\n",
    "    \n",
    "    # WITH TIME\n",
    "    data_train_time, data_test_time = split_time_dataset(data, test=0.1)\n",
    "    print(len(data_train_time))\n",
    "    print(data_train_time[0][1].ctime())\n",
    "    \n",
    "    print(len(data_test_time))\n",
    "    print(data_test_time[0][1].ctime())\n",
    "    save_dataset(data_train_time, _, data_test_time, source_path, param, dirr)\n",
    "\n",
    "if dirr == '64x64':\n",
    "    for param in params_64:\n",
    "        fin_file = param + '.pkl'\n",
    "\n",
    "        print(fin_file)\n",
    "        # loading dataset\n",
    "        with open(source_path + fin_file, 'rb') as f:\n",
    "            data = pickle.load(f, encoding='bytes')\n",
    "\n",
    "        # no valid - 90:10\n",
    "\n",
    "        # NO TIME\n",
    "        # data_train, data_valid, data_test = split_dataset(data, valid=0, test=0.1)\n",
    "        # save_dataset(data_train, data_valid, data_test, source_path, params, dirr)\n",
    "\n",
    "        # WITH TIME\n",
    "        data_train_time, data_test_time = split_time_dataset(data, test=0.1)\n",
    "        print(len(data_train_time))\n",
    "        print(data_train_time[0][1].ctime())\n",
    "\n",
    "        print(len(data_test_time))\n",
    "        print(data_test_time[0][1].ctime())\n",
    "        save_dataset(data_train_time, _, data_test_time, source_path, param, dirr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
